\documentclass[11pt,reqno]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
%\usepackage{epstopdf}
\usepackage{url}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{xspace}
%\usepackage{hyperref}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{graphicx}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{amssymb}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{balance}

\newif\ifanswers

% uncomment if you want answers to appear
%\answerstrue
% uncomment if you want answers to be suppressed
\answersfalse

\definecolor{darkgreen}{rgb}{0,.4,0}
\definecolor{orange}{rgb}{.8,.4,0}
\lstset{ 
  language=Lisp, 
  basicstyle=\small\ttfamily, 
  keywordstyle={}, 
  %commentstyle=\em \color{green}, 
  %frame=L,
  %float=tbph,
 % captionpos=b,
  showstringspaces=false, 
  keywordstyle=[3]\bf\color{orange},
  keywords=[3]{<>},
  keywordstyle=[2]\bf\color{darkgreen},
  keywords=[2]{predict,observe,assume,observe-csv},
  mathescape=true,
  stringstyle={},
           } 
\renewcommand*{\lstlistingname}{Program}

\lstnewenvironment{bodycode}[2]{\small\lstset{caption=#1,label=#2,basicstyle=\small\ttfamily}}{}
%\lstnewenvironment{code}[1][]{\lstset{caption=#1,label=#1}}{}
\lstnewenvironment{code}[2]{\small\lstset{caption=#1,label=#2}}{}

\newcommand{\+}[1]{\ensuremath{{\mathbf{#1}}}}

\newenvironment{inline}[1]{{\small\ttfamily #1}}{}

\renewcommand\vdots{%
  \vbox{\baselineskip2pt\lineskiplimit0pt\kern1pt\hbox{.}\hbox{.}\hbox{.}\kern-1pt}}

\newcommand{\bx}{{\bf x}}

\newcommand{\bz}{{\bf z}}

\title{Probabilistic Matrix Factorization \\ Via Probabilistic Programming}
\author{Frank Wood and Ziyu Wang}
%\date{}                                           % Activate to display a given date or no date



\begin{document}
\maketitle
%\section{}
%\subsection{}


{\bf Introduction}
\vspace{.5cm}

Probabilistic matrix factorization and Bayesian variants thereof came of age in the heat of the Netflix challenge problem  race.  For this reason we'll use movie ratings prediction language to describe the task of finding a probabilistic, approximate matrix factorization 

\[ \mathbf{R} \approx \mathbf{U}\mathbf{V}\] 

In such language $R_{ij}$ is the rating user $i \in \{1\ldots N\}$ gave to movie $j  \in \{1\ldots M\}$.  We would like to find matrixes $\mathbf{U} \in \mathbb{R}^{D\times N}$ and $\mathbf{V} \in \mathbb{R}^{D\times M}$ consisting of $D$-dimensional user- and movie-specific latent features given only sparse observations of the entries in $\mathbf{R}.$  One primary purpose of finding such a latent representation is to produce out of sample predictions, i.e. predicting the ratings  a user would give to movies they haven't seen given all the ratings made by everyone.

The simplest Matrix factorization generative model in \cite{salakhutdinov2008bayesian} is

\begin{align}
p(\mathbf{U}|\alpha_U) &= \prod_{i=1}^N  \mathcal{N}((U^T)_i | 0, \alpha_U^{-1}\mathbf{I})\\
p(\mathbf{V}|\alpha_V) &= \prod_{j=1}^M  \mathcal{N}((V^T)_j | 0, \alpha_V^{-1}\mathbf{I})\\
p(\mathbf{R} | \mathbf{U},  \mathbf{V}, \alpha) &= \prod_{i=1}^N \prod_{j=1}^M \left[ \mathcal{N}(R_{ij} | (U^T)_iV_j, \alpha^{-1})\right]^{I_{ij}}
\end{align}
where $X_i$ is the $i$th row of a matrix $\mathbf{X}$ and $I_{ij}=1$ if a user rated movie $j$ and $0$ otherwise.
This model straightforwardly can be used to predict $R_{ij}$ for movies $j$ that user $i$ didn't originally rate.
\newline

A provided comma separated value  (.csv) file contains ratings data for 7 movies and 10 users.  The \inline{movies.csv}\footnote{Can be downloaded from \url{http://www.robots.ox.ac.uk/~fwood/anglican/teaching/mlss2014/bayesian_matrix_factorization/movies.csv}} file contains three columns.  The first is an integer user id.  The second is an integer movie id.  The third is the given integer rating. The data are a very small subset from \url{http://grouplens.org/datasets/movielens/}.

An example row is \inline{9,1,2} which means user 9 rated movie 1 a 2.  Note that user 9 did not rate movie 1 in the dataset.

The movies -- (ID) -- are

\begin{enumerate}
\item Toy Story (1995)
\item Twelve Monkeys (1995)
\item Seven (Se7en) (1995)
\item From Dusk Till Dawn (1996)
\item Four Weddings and a Funeral (1994)
\item Jurassic Park (1993)
\item Pretty Woman (1990)
\end{enumerate}


\vspace{.5cm}

\noindent {\bf Questions : }
\vspace{.5cm}

{\bf (1)} Complete the generative model code below corresponding to the generative model above starting from the following scaffolding and use it to predict the rating user $9$ might give to movie $1$.

\begin{code}{}{}
[assume dot-product (lambda (u v) 
                      (if (= (count u) 0) 0 
                          (+ (* (first u) (first v)) 
                             (dot-product (rest u) (rest v)))))]
[assume repeatedly (lambda (N func)
    (if (= N 0)
        (list)
        (cons (func) (repeatedly (- N 1) func))))]

[assume D 4] ; this is the shared length of rows of U / columns of V 
[assume sigma 0.01]
[assume sigma-U 1]
[assume sigma-V 1]
[assume get-row-U 
    (mem (lambda (m) (repeatedly D <$\cdots$>)))]
[assume get-col-V 
    (mem (lambda (n) (repeatedly D <$\cdots$>)))]

[observe-csv "movies.csv" 
     (<$\cdots$> (dot-product (get-row-U $\$$1) (get-col-V $\$$2)) <$\cdots$>) $\$$3]
[predict <$\cdots$> ]
\end{code}

where \inline{<$\cdots$>} needs to be filled in by you.

\ifanswers
\begin{quotation}
 {\bf Answer } 
 \begin{code}{}{}
[assume dot-product (lambda (u v) 
                      (if (= (count u) 0) 0 
                          (+ (* (first u) (first v)) 
                             (dot-product (rest u) (rest v)))))]
[assume repeatedly (lambda (N func)
    (if (= N 0)
        (list)
        (cons (func) (repeatedly (- N 1) func))))]

[assume D 4] ; this is the shared length of rows of U / columns of V 
[assume sigma 0.01]
[assume sigma-U 1]
[assume sigma-V 1]
[assume get-row-U 
    (mem (lambda (m) (repeatedly D (lambda () (normal 0 sigma-U) ))))]
[assume get-col-V 
    (mem (lambda (n) (repeatedly D (lambda () (normal 0 sigma-V) ))))]

[observe-csv "movies.csv" 
     (normal (dot-product (get-row-U $\$$1) (get-col-V $\$$2)) sigma) $\$$3]
[predict  (dot-product (get-row-U 9) (get-col-V 1)) ]
\end{code}
\end{quotation}
\fi

\vspace{1cm}
{\bf (2)} Change the generative model to be ``open universe,'' i.e. make it allow for an unknown number of latent features.

\ifanswers
\begin{quotation}
 {\bf Answer } 
 Change the line
 \begin{code}{}{}
 [assume D 4]
\end{code}
to
 \begin{code}{}{}
 [assume D (poisson 4)]
\end{code}
\end{quotation}
\fi


\vspace{1cm}
{\bf (3)} Change the generative model to so that it does non-negative matrix factorization.

\ifanswers
\begin{quotation}
 {\bf Answer } 
 An answer can be written by changing the observation model to be Poisson.  The requires a positive rate argument which can be ensured by using Gamma priors for all entries of latent matrixes $\mathbf U$ and $\mathbf V$.
 \end{quotation}


 \begin{code}{}{}
[assume dot-product (lambda (u v) 
                      (if (= (count u) 0) 0 
                          (+ (* (first u) (first v)) 
                             (dot-product (rest u) (rest v)))))]
[assume repeatedly (lambda (N func)
    (if (= N 0)
        (list)
        (cons (func) (repeatedly (- N 1) func))))]

[assume D 4] ; this is the shared length of rows of U / columns of V 
[assume sigma 0.01]
[assume sigma-U 1]
[assume sigma-V 1]
[assume get-row-U (mem (lambda (m) 
                 (repeatedly D (lambda () (gamma 1 sigma-U) ))))]
[assume get-col-V (mem (lambda (n) 
                  (repeatedly D (lambda () (gamma 1 sigma-V) ))))]

[observe-csv "movies.csv" 
           (poisson (dot-product (get-row-U $\$$1) (get-col-V $\$$2))) $\$$3]
[predict (poisson (dot-product (get-row-U 9) (get-col-V 1)))]
\end{code}
\fi


\vspace{1cm}
{\bf (4)} Why might the generic inference strategies (RDB and PMCMC) not mix well for this model?  Suggestion: diagram an execution trace tree.

\ifanswers
\vspace{.5cm}
\begin{quotation}
 {\bf Answer } While randomness is generated lazily in this program, it remains the case that the first time a row of ${\bf U}^T$ and a column of ${\bf V}$ are needed to compute the mean for $R_{ij}$ {\em all} entries of those rows and columns are sampled simultaneously.  In a sequential Monte Carlo inference algorithm the next time a weight is applied is when row $i$ or column $j$ is accessed again.  The chance of sampling a good set of latent features from the prior is very low (i.e. one that yields high weights in all subsequent \inline{observe}'s).  For a single-site MCMC algorithm to really work a block update of a row or column needs to be performed.  Modifying a single variable at once will, like usual, take a long time to converge.
 \end{quotation}
\fi

\bibliographystyle{plainnat}

\bibliography{../refs}




\end{document}  